services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT

  producer:
    build: ./kafka
    volumes:
      - ./data/flights:/data/flights
    depends_on:
      - kafka
    environment:
      PYTHONUNBUFFERED: 1

  spark:
    image: bitnami/spark:3.5.1
    container_name: spark
    build:
      context: ./spark
      dockerfile: Dockerfile
    environment:
      - SPARK_MODE=driver
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark:/opt/spark
      - ./data:/opt/data
    depends_on:
      - producer
      - kafka
      - mongodb
    ports:
      - "4040:4040"
    command:
      [
        "spark-submit",
        "--conf",
        "spark.jars.ivy=/tmp/.ivy2",
        "--packages",
        "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.mongodb.spark:mongo-spark-connector_2.12:10.4.0", # packages
        "/opt/spark/spark_receiver.py",
        "60",
        "30" # number of flights for the anomalies to save
      ]

  mongodb:
    image: mongo:6.0
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongo-data:/data/db
    restart: unless-stopped

volumes:
  mongo-data:
